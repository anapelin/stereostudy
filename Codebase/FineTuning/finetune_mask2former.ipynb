{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58aec31e",
   "metadata": {},
   "source": [
    "# Mask2Former Fine-tuning for Contrail Segmentation\n",
    "\n",
    "This notebook fine-tunes a pre-trained Mask2Former model on the contrail segmentation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602de950",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-13 16:16:37.846629: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-13 16:16:37.926103: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-01-13 16:16:39.783303: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model: facebook/mask2former-swin-base-coco-instance\n",
      "Dataset directory: /data/common/STEREOSTUDYIPSL/Codebase/FineTuning/dataset/D-imageWithAnnotation/D-imageWithAnnotation\n",
      "Output directory: /data/common/STEREOSTUDYIPSL/Codebase/FineTuning\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "from pathlib import Path\n",
    "from transformers import (\n",
    "    Mask2FormerImageProcessor,\n",
    "    Mask2FormerForUniversalSegmentation,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "import torch\n",
    "import albumentations as A\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "\n",
    "# Configuration\n",
    "run_id = \"contrail_segmentation\"\n",
    "task = \"instance\"  # Instance segmentation for contrail detection\n",
    "batch_size = 2  # Adjust based on GPU memory\n",
    "num_train_epochs = 5\n",
    "model_size = \"base\"\n",
    "\n",
    "# Dataset paths\n",
    "dataset_dir = Path(\"/data/common/STEREOSTUDYIPSL/Codebase/FineTuning/dataset/D-imageWithAnnotation/D-imageWithAnnotation\")\n",
    "model_dir = Path(\"/data/common/STEREOSTUDYIPSL/Codebase/FineTuning\")\n",
    "base_model = f\"facebook/mask2former-swin-{model_size}-coco-{task}\"\n",
    "\n",
    "print(f\"Base model: {base_model}\")\n",
    "print(f\"Dataset directory: {dataset_dir}\")\n",
    "print(f\"Output directory: {model_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da263406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "Categories found: {0: 'contrail', 1: 'contrail maybe', 2: 'contrail old', 3: 'contrail veryold', 4: 'contrail young', 5: 'parasite', 6: 'sun', 7: 'unknow'}\n",
      "Number of categories: 8\n",
      "Number of images: 1600\n"
     ]
    }
   ],
   "source": [
    "# Load COCO annotations and setup categories\n",
    "coco = COCO(str(dataset_dir / \"annotations.coco.json\"))\n",
    "\n",
    "# Get categories from the dataset\n",
    "cat_ids = coco.getCatIds()\n",
    "categories = coco.loadCats(cat_ids)\n",
    "\n",
    "# Create id2label mapping (model expects 0-indexed labels)\n",
    "id2label = {idx: cat[\"name\"] for idx, cat in enumerate(categories)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "print(f\"Categories found: {id2label}\")\n",
    "print(f\"Number of categories: {len(categories)}\")\n",
    "print(f\"Number of images: {len(coco.getImgIds())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7b3e3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1568 valid images out of 1600 total\n",
      "Training images: 1254\n",
      "Validation images: 314\n"
     ]
    }
   ],
   "source": [
    "# Split dataset into train and validation sets\n",
    "def split_coco_dataset(coco, img_dir, train_size=0.8, seed=42):\n",
    "    \"\"\"Split COCO dataset into train and validation sets, filtering for existing images.\"\"\"\n",
    "    random.seed(seed)\n",
    "    img_ids = coco.getImgIds()\n",
    "    \n",
    "    # Filter to only include images that exist on disk\n",
    "    valid_img_ids = []\n",
    "    for img_id in img_ids:\n",
    "        img_info = coco.loadImgs(img_id)[0]\n",
    "        img_path = img_dir / img_info[\"file_name\"]\n",
    "        if img_path.exists():\n",
    "            valid_img_ids.append(img_id)\n",
    "    \n",
    "    print(f\"Found {len(valid_img_ids)} valid images out of {len(img_ids)} total\")\n",
    "    \n",
    "    random.shuffle(valid_img_ids)\n",
    "    \n",
    "    split_idx = int(len(valid_img_ids) * train_size)\n",
    "    train_ids = valid_img_ids[:split_idx]\n",
    "    val_ids = valid_img_ids[split_idx:]\n",
    "    \n",
    "    return train_ids, val_ids\n",
    "\n",
    "train_img_ids, val_img_ids = split_coco_dataset(coco, dataset_dir, train_size=0.8, seed=42)\n",
    "print(f\"Training images: {len(train_img_ids)}\")\n",
    "print(f\"Validation images: {len(val_img_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3344c5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom dataset class for instance segmentation\n",
    "class InstanceSegmentationDataset(Dataset):\n",
    "    \"\"\"Dataset class for instance segmentation with Mask2Former.\"\"\"\n",
    "    \n",
    "    def __init__(self, coco, img_ids, img_dir, processor, transform=None):\n",
    "        self.coco = coco\n",
    "        self.img_ids = img_ids\n",
    "        self.img_dir = Path(img_dir)\n",
    "        self.processor = processor\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.img_ids[idx]\n",
    "        img_info = self.coco.loadImgs(img_id)[0]\n",
    "        \n",
    "        # Load image\n",
    "        img_path = self.img_dir / img_info[\"file_name\"]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = np.array(image, dtype=np.uint8)  # Ensure uint8 format\n",
    "        \n",
    "        # Get annotations for this image\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "        \n",
    "        # Create instance masks and class labels\n",
    "        masks = []\n",
    "        class_labels = []\n",
    "        \n",
    "        for ann in anns:\n",
    "            # Get binary mask from annotation\n",
    "            mask = self.coco.annToMask(ann)\n",
    "            if mask.sum() > 0:  # Only add non-empty masks\n",
    "                masks.append(mask.astype(np.uint8))\n",
    "                # Map category id to 0-indexed label\n",
    "                cat_id = ann[\"category_id\"]\n",
    "                class_labels.append(cat_id)\n",
    "        \n",
    "        # Apply augmentations if provided\n",
    "        if self.transform is not None and len(masks) > 0:\n",
    "            transformed = self.transform(image=image, masks=masks)\n",
    "            image = transformed[\"image\"]\n",
    "            masks = transformed[\"masks\"]\n",
    "        \n",
    "        # If no masks, create dummy mask\n",
    "        if len(masks) == 0:\n",
    "            masks = [np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8)]\n",
    "            class_labels = [0]  # Default to first class\n",
    "        \n",
    "        # Stack masks into (num_instances, H, W) format\n",
    "        instance_masks = np.stack(masks, axis=0).astype(np.uint8)\n",
    "        \n",
    "        # Convert class labels to tensor\n",
    "        class_labels_tensor = torch.tensor(class_labels, dtype=torch.long)\n",
    "        \n",
    "        # Process image with Mask2Former processor\n",
    "        # Use input_data_format to explicitly specify HWC format\n",
    "        inputs = self.processor(\n",
    "            images=image,\n",
    "            return_tensors=\"pt\",\n",
    "            input_data_format=\"channels_last\",  # Explicitly specify HWC format\n",
    "        )\n",
    "        \n",
    "        # Convert masks to tensors\n",
    "        mask_labels = torch.from_numpy(instance_masks).float()\n",
    "        \n",
    "        return {\n",
    "            \"pixel_values\": inputs[\"pixel_values\"].squeeze(0),\n",
    "            \"pixel_mask\": inputs[\"pixel_mask\"].squeeze(0),\n",
    "            \"class_labels\": class_labels_tensor,\n",
    "            \"mask_labels\": mask_labels,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b7d9518",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/dataiku/.conda/envs/mask2former/lib/python3.10/site-packages/transformers/image_processing_base.py:417: UserWarning: The following named arguments are not valid for `Mask2FormerImageProcessor.__init__` and were ignored: '_max_size', 'reduce_labels'\n",
      "  image_processor = cls(**image_processor_dict)\n",
      "Some weights of Mask2FormerForUniversalSegmentation were not initialized from the model checkpoint at facebook/mask2former-swin-base-coco-instance and are newly initialized because the shapes did not match:\n",
      "- class_predictor.bias: found shape torch.Size([81]) in the checkpoint and torch.Size([9]) in the model instantiated\n",
      "- class_predictor.weight: found shape torch.Size([81, 256]) in the checkpoint and torch.Size([9, 256]) in the model instantiated\n",
      "- criterion.empty_weight: found shape torch.Size([81]) in the checkpoint and torch.Size([9]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: facebook/mask2former-swin-base-coco-instance\n",
      "Number of parameters: 106,885,697\n"
     ]
    }
   ],
   "source": [
    "# Setup image processor and model\n",
    "processor = Mask2FormerImageProcessor.from_pretrained(\n",
    "    base_model,\n",
    "    do_resize=True,\n",
    "    size={\"height\": 512, \"width\": 512},  # Resize to manageable size\n",
    "    do_rescale=True,\n",
    "    do_normalize=True,\n",
    "    ignore_index=255,\n",
    ")\n",
    "\n",
    "# Load model with custom class mappings\n",
    "model = Mask2FormerForUniversalSegmentation.from_pretrained(\n",
    "    base_model,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True,  # Class count differs from COCO\n",
    ")\n",
    "\n",
    "print(f\"Model loaded: {base_model}\")\n",
    "print(f\"Number of parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dac34058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 1254\n",
      "Validation dataset size: 314\n"
     ]
    }
   ],
   "source": [
    "# Define augmentations\n",
    "transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = InstanceSegmentationDataset(\n",
    "    coco=coco,\n",
    "    img_ids=train_img_ids,\n",
    "    img_dir=dataset_dir,\n",
    "    processor=processor,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "val_dataset = InstanceSegmentationDataset(\n",
    "    coco=coco,\n",
    "    img_ids=val_img_ids,\n",
    "    img_dir=dataset_dir,\n",
    "    processor=processor,\n",
    "    transform=None,  # No augmentation for validation\n",
    ")\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3e751c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom collate function for batching\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function to batch samples properly.\"\"\"\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in batch])\n",
    "    pixel_mask = torch.stack([example[\"pixel_mask\"] for example in batch])\n",
    "    class_labels = [example[\"class_labels\"] for example in batch]\n",
    "    mask_labels = [example[\"mask_labels\"] for example in batch]\n",
    "\n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"pixel_mask\": pixel_mask,\n",
    "        \"class_labels\": class_labels,\n",
    "        \"mask_labels\": mask_labels,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7ce5229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training configuration:\n",
      "  - Batch size: 2\n",
      "  - Learning rate: 1.25e-05\n",
      "  - Epochs: 2\n",
      "  - Output dir: /data/common/STEREOSTUDYIPSL/Codebase/FineTuning/checkpoints\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=str(model_dir / \"checkpoints\"),\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    logging_steps=100,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    learning_rate=(batch_size / 16.0) * 1e-4,\n",
    "    fp16=True,  # Mixed-precision training\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_num_workers=4,\n",
    ")\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStoppingCallback(early_stopping_patience=3)\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  - Batch size: {batch_size}\")\n",
    "print(f\"  - Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  - Epochs: {num_train_epochs}\")\n",
    "print(f\"  - Output dir: {training_args.output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ba597b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialized. Starting training...\n",
      "Total training steps: 1254\n"
     ]
    }
   ],
   "source": [
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=collate_fn,\n",
    "    callbacks=[early_stopping],\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized. Starting training...\")\n",
    "print(f\"Total training steps: {len(train_dataset) // batch_size * num_train_epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "571e73a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1254' max='1254' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1254/1254 04:12, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>46.311200</td>\n",
       "      <td>46.055614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>42.759400</td>\n",
       "      <td>44.426640</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1254, training_loss=47.37378630569677, metrics={'train_runtime': 253.2772, 'train_samples_per_second': 9.902, 'train_steps_per_second': 4.951, 'total_flos': 1.2642947857058365e+18, 'train_loss': 47.37378630569677, 'epoch': 2.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d6902198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best checkpoint: /data/common/STEREOSTUDYIPSL/Codebase/FineTuning/checkpoints/checkpoint-1000\n",
      "Model saved to: /data/common/STEREOSTUDYIPSL/Codebase/FineTuning/contrail_segmentation\n",
      "Processor saved to: /data/common/STEREOSTUDYIPSL/Codebase/FineTuning/contrail_segmentation\n"
     ]
    }
   ],
   "source": [
    "# Save the best model\n",
    "import shutil\n",
    "\n",
    "# Get path to best checkpoint\n",
    "best_ckpt_path = Path(trainer.state.best_model_checkpoint)\n",
    "best_path = model_dir / run_id\n",
    "\n",
    "print(f\"Best checkpoint: {best_ckpt_path}\")\n",
    "\n",
    "# Remove existing directory if it exists\n",
    "if best_path.exists():\n",
    "    shutil.rmtree(best_path)\n",
    "\n",
    "# Rename best checkpoint to final model directory\n",
    "best_ckpt_path.rename(best_path)\n",
    "\n",
    "print(f\"Model saved to: {best_path}\")\n",
    "\n",
    "# Also save the processor\n",
    "processor.save_pretrained(best_path)\n",
    "print(f\"Processor saved to: {best_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8803039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up checkpoints directory (optional)\n",
    "checkpoints_dir = model_dir / \"checkpoints\"\n",
    "if checkpoints_dir.exists():\n",
    "    shutil.rmtree(checkpoints_dir)\n",
    "    print(f\"Cleaned up checkpoints directory: {checkpoints_dir}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Training complete!\")\n",
    "print(f\"Final model saved to: {best_path}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445f47ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5509040a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mask2former",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
